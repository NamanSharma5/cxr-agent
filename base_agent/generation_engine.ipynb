{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.hf_cache' ## THIS HAS TO BE BEFORE YOU IMPORT TRANSFORMERS\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from my_secrets import LLAMA3_INSTRUCT_ACCESS_TOKEN\n",
    "from agent_utils import select_best_gpu\n",
    "\n",
    "from pathology_detector import PathologyDetector, CheXagentVisionTransformerPathologyDetector\n",
    "from pathology_sets import Pathologies\n",
    "\n",
    "from phrase_grounder import PhraseGrounder, BioVilTPhraseGrounder\n",
    "\n",
    "class GenerationEngine(ABC):\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str] = None, image_path = None):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def contextualise_model(image_path: Path, pathology_detector: PathologyDetector, phrase_grounder: Optional[PhraseGrounder] = None, examples = True) -> str:\n",
    "        \n",
    "        pathology_detection_threshold = 0.5\n",
    "\n",
    "        if pathology_detector is not None:\n",
    "            pathology_confidences = pathology_detector.detect_pathologies(image_path, threshold = pathology_detection_threshold)\n",
    "            # print(pathology_confidences)\n",
    "        else:\n",
    "            return RuntimeError(\"Pathology detector not provided\")\n",
    "        \n",
    "        #### PROMPT PIPELINE ###\n",
    "        if len(pathology_confidences) == 0:\n",
    "            system_prompt = \"\"\" You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. However there is insufficient data make any comments. Just mention it is possible there are no findings and this should be double checked by a radiologist.\"\"\"\n",
    "            image_context_prompt = f\"\"\"No pathologies were detected in the chest X-ray image. The user will now interact with you.\"\"\"\n",
    "            return system_prompt, image_context_prompt\n",
    "        \n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer CONCISELY and professionally as a radiologist would. You should not be confident unless the data is confident. Use language that reflects the confidence of the data.\"\"\"\n",
    "        image_context_prompt_final_part = \"\"\"\n",
    "            This understanding is crucial for accurately processing and responding to queries on the chest X-ray analysis. Structure your answers based on confidence and pathologies.\n",
    "            Double check before you submit your response to ensure you have factored in all the data and followed my instructions carefully.\n",
    "            You will now interact with the user.\n",
    "            \"\"\"\n",
    "\n",
    "        if phrase_grounder is None:\n",
    "            image_context_prompt = f\"\"\"\n",
    "                You are being provided with data derived from analyzing a chest X-ray, which includes findings on potential pathologies alongside their confidence levels.\n",
    "                This information comes from a specialized diagnostic tools.\n",
    "                It's important to recognize how to interpret this when responding to queries:\n",
    "\n",
    "                Pathology Detection with Confidence Scores:\n",
    "                {pathology_confidences}\n",
    "                Please note the closer the value to 1, the more likely the pathology is present in the image. \n",
    "\n",
    "                It is important to factor medical knowledge and the specifics of each case, if supplied, into your responses.\n",
    "                {image_context_prompt_final_part}\n",
    "                \"\"\"\n",
    "\n",
    "        else:\n",
    "            pathologies = [pathology for pathology, confidence in pathology_confidences.items() if confidence > pathology_detection_threshold]\n",
    "            grounded_pathologies_confidences = phrase_grounder.get_pathology_lateral_position(image_path, pathologies)\n",
    "            # print(grounded_pathologies_confidences)\n",
    "            if len(grounded_pathologies_confidences) == 0:\n",
    "                grounded_pathologies_confidences = \"No lateral positions could be confidently determined for any pathologies detected.\"\n",
    "\n",
    "            image_context_prompt = f\"\"\"\n",
    "            You are being provided with data derived from analyzing a chest X-ray, which includes findings on potential pathologies alongside their confidence levels and, separately, possible lateral locations of these pathologies with their own confidence scores.\n",
    "            This information comes from two specialized diagnostic tools.\n",
    "            It's important to recognize how to interpret these datasets together when responding to queries:\n",
    "\n",
    "            Pathology Detection with Confidence Scores:\n",
    "            {pathology_confidences}\n",
    "\n",
    "            Phrase Grounding Locations with Confidence Scores:\n",
    "            {grounded_pathologies_confidences}\n",
    "\n",
    "            This separate dataset provides potential lateral locations for some of the detected pathologies, each with its own confidence score, indicating the model's certainty about each pathology's location.\n",
    "            For instance, left Pleural Effusion is listed with a confidence score of 0.53, suggesting the location of Pleural Effusion to be on the left side with moderate confidence.\n",
    "\n",
    "            When you interact with end users, remember:\n",
    "\n",
    "            - A pathology and its lateral location (e.g., Pleural Effusion and left Pleural Effusion) are part of the same finding. The location attribute is an additional detail about where the pathology is likely found, not an indicator of a separate pathology. \n",
    "            - Synthesize the pathology detection and localization data. DO NOT TALK ABOUT THEM SEPERATELY. {\"Here is a model example, 'Highly likely there is Pleural Effusion (detection confidence: 0.80), and it is possibly on the left side (localisation confidence: 0.53).'\" if examples else \"\"}\n",
    "            - Confidence scores from the pathology detection and phrase grounding tools are not directly comparable. They serve as indicators of confidence within their respective contexts of pathology detection and localisation.\n",
    "            - A missing lateral location does not imply the absence of a pathology; it indicates the localisation could not be confidently determined.\n",
    "            - If there is any discrepancy between the pathology detection and phrase grounding tools, detection data takes precedence as it more reliably identifies pathologies.\n",
    "\n",
    "            It is important to factor medical knowledge and the specifics of each case, if supplied, into your responses. For example, pathologies located on both sides are called bilateral. Heart related observations are usually on the left/ middle.\n",
    "            {image_context_prompt_final_part}\n",
    "            \"\"\"\n",
    "            \n",
    "        return system_prompt, image_context_prompt\n",
    "\n",
    "\n",
    "\n",
    "class Llama3Generation(GenerationEngine):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map= select_best_gpu() ,\n",
    "            token=LLAMA3_INSTRUCT_ACCESS_TOKEN,\n",
    "        )\n",
    "\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str] = None):\n",
    "        def format_output(output_text: str) -> str:\n",
    "            return \"\\n\".join(output_text.split(\". \"))  # print output text with each sentence on a new line\n",
    "\n",
    "        terminators = [\n",
    "            self.pipeline.tokenizer.eos_token_id,\n",
    "            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        if user_prompt is not None:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": image_context_prompt +\"\\n\" + user_prompt},\n",
    "            ]\n",
    "\n",
    "            prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            output_text = outputs[0][\"generated_text\"][len(prompt):]\n",
    "            print(format_output(output_text))\n",
    "            return outputs[0][\"generated_text\"][len(prompt):]\n",
    "        \n",
    "        else:\n",
    "            # setup chat loop\n",
    "            chat_history = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": image_context_prompt},\n",
    "            ]\n",
    "\n",
    "            user_prompt = \"\"\n",
    "            while user_prompt != \"exit\":\n",
    "                prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "                    chat_history, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                outputs = self.pipeline(\n",
    "                    prompt,\n",
    "                    max_new_tokens=512,\n",
    "                    eos_token_id=terminators,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "\n",
    "                output_text = outputs[0][\"generated_text\"][len(prompt):]\n",
    "                chat_history.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "                print(format_output(output_text))\n",
    "\n",
    "                user_prompt = input(\"User: \")\n",
    "                print(f\"\\n{user_prompt}\")\n",
    "                chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "            return chat_history\n",
    "\n",
    "class CheXagentLanguageModelGeneration(GenerationEngine):\n",
    "\n",
    "    def __init__(self, processor, model, generation_config, device, dtype):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.generation_config = generation_config\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str] = None):\n",
    "        if user_prompt is None:\n",
    "            return RuntimeError(\"User prompt not provided - Chat mode CURRENTLY not supported with CheXagent Language Model\")\n",
    "        inputs = self.processor(\n",
    "        images=None, text=f\"[INST]{image_context_prompt}[/INST] USER: <s>{user_prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "        ).to(device=self.device)\n",
    "        output = self.model.generate(**inputs, generation_config=self.generation_config,language_only = True)[0]\n",
    "        response = self.processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(response)\n",
    "        return response\n",
    "    \n",
    "\n",
    "class CheXagentEndToEndGeneration(GenerationEngine):\n",
    "\n",
    "    def __init__(self, processor, model, generation_config, device, dtype):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.generation_config = generation_config\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str],image_path: Path):\n",
    "        images = [Image.open(image_path).convert(\"RGB\")]\n",
    "        inputs = self.processor(\n",
    "            images=images, text=f\" USER: <s>{user_prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "        ).to(device=self.device, dtype=self.dtype)\n",
    "        # print(inputs.keys())\n",
    "        output = self.model.generate(**inputs, generation_config=self.generation_config)[0]\n",
    "        response = self.processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(response)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX 6000 Ada Generation, Free memory: 46315 MB\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation, Free memory: 48643 MB\n",
      "Selecting GPU 1 with 48643 MB free memory, Device = cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b4b4329d9543ce8714816e4b18cf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagent Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CXRBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type cxr-bert. This is not supported for all configurations of models and can yield errors.\n",
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at microsoft/BiomedVLP-BioViL-T were not used when initializing CXRBertModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CXRBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CXRBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation, Free memory: 45885 MB\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation, Free memory: 31391 MB\n",
      "Selecting GPU 0 with 45885 MB free memory, Device = cuda:0\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation, Free memory: 45313 MB\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation, Free memory: 31391 MB\n",
      "Selecting GPU 0 with 45313 MB free memory, Device = cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b05a897f7f4ca98a60c2e3e6c90d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pathology_detector = CheXagentVisionTransformerPathologyDetector(pathologies=Pathologies.CHEXPERT)\n",
    "phrase_grounder = BioVilTPhraseGrounder(detection_threshold=0.5)\n",
    "l3 = Llama3Generation()\n",
    "cheXagent_lm = CheXagentLanguageModelGeneration(pathology_detector.processor, pathology_detector.model, pathology_detector.generation_config, pathology_detector.device, pathology_detector.dtype)\n",
    "cheXagent_e2e = CheXagentEndToEndGeneration(pathology_detector.processor, pathology_detector.model, pathology_detector.generation_config, pathology_detector.device, pathology_detector.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.hf_cache/modules/transformers_modules/4934e91451945c8218c267aae9c34929a7677829/processing_chexagent.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(pixel_values) for pixel_values in encoding_image_processor[\"pixel_values\"]]\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image 0\n",
      "image_path='test/patient64765/study1/view1_frontal.jpg'\n",
      "L3 agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the chest X-ray image provided, I did not detect any obvious pathologies or abnormalities.\n",
      "\n",
      "It is possible that the image may not have captured any significant findings, and I would recommend that a radiologist review the image to confirm the absence of any notable features\n",
      "Additionally, the image may not have been taken with the correct positioning or technique, which could also impact the accuracy of the interpretation.\n",
      "\n",
      "In the absence of any obvious findings, I would recommend that the patient's clinical history and any relevant symptoms be taken into consideration to determine if further imaging or evaluation is necessary.\n",
      "\n",
      "Please note that this is a preliminary report and should not be used as a definitive diagnosis without further review and confirmation by a radiologist.\n",
      "CheXagent agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pathologies were detected in the chest x-ray image.\n",
      "CheXagent:\n",
      "The heart size is normal. The mediastinal and hilar contours are normal. The pulmonary vasculature is normal. The lungs are clear. No pleural effusion or pneumothorax is seen. There are no acute osseous abnormalities.\n",
      " -----------------\n",
      "image_path='test/patient64766/study1/view1_frontal.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3 agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the analysis of the chest X-ray image, the following findings are reported:\n",
      "\n",
      "A pneumothorax is detected with a moderate confidence level of 0.53\n",
      "No lateral location could be confidently determined for this pathology.\n",
      "\n",
      "A support device is also detected with a high confidence level of 0.9\n",
      "No lateral location could be confidently determined for this pathology.\n",
      "\n",
      "In summary, a pneumothorax is detected with moderate confidence, and a support device is detected with high confidence, but the lateral location of both pathologies could not be confidently determined.\n",
      "CheXagent agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chest x-ray image shows the presence of a pneumothorax on the left side with a confidence score of 0.53. However, no lateral positions could be confidently determined for any pathologies detected.\n",
      "CheXagent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In comparison with the study of ___, there is little change and no evidence of acute cardiopulmonary disease. No pneumonia, vascular congestion, or pleural effusion. Central catheter remains in place.\n",
      " -----------------\n",
      "image_path='test/patient64766/study1/view2_lateral.jpg'\n",
      "L3 agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided chest X-ray image, I have reviewed the film and found no obvious abnormalities or pathologies\n",
      "The lungs appear to be clear and well-expanded, with no visible infiltrates, masses, or effusions.\n",
      "\n",
      "It is possible that there may be no significant findings on this image, but I would recommend that a radiologist review the image to confirm this assessment\n",
      "Additionally, any potential findings may be subtle and require further evaluation with additional imaging studies or clinical correlation.\n",
      "\n",
      "In the absence of any obvious abnormalities, the patient's lungs appear to be normal and without any significant pathology\n",
      "However, I would emphasize that a radiologist's review is necessary to confirm this assessment and provide a definitive report.\n",
      "\n",
      "Please note that a radiologist's interpretation is always the most accurate and reliable, and I would recommend that they review the image to provide a comprehensive and definitive report.\n",
      "CheXagent agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pathologies were detected in the chest x-ray image.\n",
      "CheXagent:\n",
      "The heart size is normal. The mediastinal and hilar contours are normal. The pulmonary vasculature is normal. The lungs are clear. No pleural effusion or pneumothorax is seen.\n",
      " -----------------\n",
      "image_path='test/patient64767/study1/view1_frontal.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3 agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the analysis of the chest X-ray image, the following findings are reported:\n",
      "\n",
      "There is moderate confidence (0.91) in the presence of Lung Opacity\n",
      "Additionally, there is high confidence (0.97) in the presence of Pleural Effusion\n",
      "The location of this Pleural Effusion is likely on the left side, with a confidence score of 0.53.\n",
      "\n",
      "There is also moderate confidence (0.54) in the presence of Atelectasis\n",
      "However, the location of this Atelectasis could not be confidently determined.\n",
      "\n",
      "Lastly, there is high confidence (0.87) in the presence of Support Devices, which are likely located on both the left and right sides, with confidence scores of 0.54 and 0.6, respectively.\n",
      "\n",
      "In summary, the chest X-ray image shows Lung Opacity with moderate confidence, Pleural Effusion with high confidence and likely located on the left side, Atelectasis with moderate confidence but uncertain location, and Support Devices with high confidence and likely located bilaterally.\n",
      "CheXagent agent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chest x-ray image shows several pathologies, including Lung Opacity, Atelectasis, Pleural Effusion, and Support Devices. The Support Devices are located on the left and right sides with moderate and high confidence, respectively.\n",
      "CheXagent:\n",
      "In comparison with the study of ___, the monitoring and support devices are essentially unchanged. Continued enlargement of the cardiac silhouette with mild elevation of pulmonary venous pressure. Retrocardiac opacification is consistent with volume loss in the left lower lobe and small pleural effusion.\n",
      " -----------------\n"
     ]
    }
   ],
   "source": [
    "chexpert_test_csv_path = Path(\"/vol/biodata/data/chest_xray/CheXpert-v1.0-small/CheXpert-v1.0-small/test.csv\")\n",
    "chexpert_test_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/CheXpert/small/\")\n",
    "\n",
    "with open(chexpert_test_csv_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0].split(\",\")[1:]\n",
    "    # print(header)\n",
    "    for i, line in enumerate(lines[40:]):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Collecting image {i}\")\n",
    "\n",
    "        image_path = line.split(\",\")[0]\n",
    "        print(f\"{image_path=}\")\n",
    "        image_path = chexpert_test_path / image_path\n",
    "        system_prompt, image_context_prompt = GenerationEngine.contextualise_model(image_path=image_path,pathology_detector=pathology_detector, phrase_grounder=phrase_grounder, examples=False)\n",
    "        # print(system_prompt)\n",
    "        # print(image_context_prompt)\n",
    "\n",
    "        user_prompt = f\"\"\"Write a report on the chest x-ray image.\"\"\"\n",
    "        print(\"L3 agent:\")\n",
    "        l3.generate_model_output(system_prompt, image_context_prompt,user_prompt=user_prompt)\n",
    "        print(\"CheXagent agent:\")\n",
    "        cheXagent_lm.generate_model_output(system_prompt, image_context_prompt,user_prompt=user_prompt)\n",
    "        print(\"CheXagent:\")\n",
    "        cheXagent_e2e.generate_model_output(system_prompt, image_context_prompt,user_prompt=user_prompt, image_path=image_path)\n",
    "        print(f\" -----------------\")\n",
    "        if i == 3: \n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
