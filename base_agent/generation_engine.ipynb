{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.hf_cache' ## THIS HAS TO BE BEFORE YOU IMPORT TRANSFORMERS\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from my_secrets import LLAMA3_INSTRUCT_ACCESS_TOKEN\n",
    "from agent_utils import select_best_gpu\n",
    "\n",
    "from pathology_detector import PathologyDetector, CheXagentVisionTransformerPathologyDetector\n",
    "from pathology_sets import Pathologies\n",
    "\n",
    "class GenerationEngine(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_report(self, image_path: Path, prompt: Optional[str], output_dir: Optional[str]) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Llama3Generation(GenerationEngine):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map= select_best_gpu() ,\n",
    "            token=LLAMA3_INSTRUCT_ACCESS_TOKEN,\n",
    "        )\n",
    "\n",
    "\n",
    "    def generate_report(self, image_path: Path, prompt: Optional[str], pathology_detector = None) -> str:\n",
    "        \n",
    "        if pathology_detector is not None:\n",
    "            pathology_confidences = pathology_detector.detect_pathologies(image_path, threshold = 0.5)\n",
    "        else:\n",
    "            pathology_confidences = { 'Consolidation': 0.21995275,'Pulmonary fibrosis': 0.49335942, 'No finding': 0.1746179}\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer concisely and in a professional manner.\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Using specialised pathology detection tools,\n",
    "        you are given the following pathology detection results for a chest X-ray:\n",
    "        {pathology_confidences}\n",
    "\n",
    "        Please note the closer the value to 1, the more likely the pathology is present in the image. \n",
    "        Write up a findings section based on these observations\"\"\"\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt },\n",
    "        ]\n",
    "\n",
    "        prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        terminators = [\n",
    "            self.pipeline.tokenizer.eos_token_id,\n",
    "            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        outputs = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "        return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4090, Free memory: 24176 MB\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Free memory: 24203 MB\n",
      "Selecting GPU 1 with 24203 MB free memory, Device = cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812c3bfca2b74bbfb997248aba22c2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4090, Free memory: 24176 MB\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Free memory: 8369 MB\n",
      "Selecting GPU 0 with 24176 MB free memory, Device = cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6091db0d58bd48128d7f49f948b77e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagent Model loaded\n"
     ]
    }
   ],
   "source": [
    "l3 = Llama3Generation()\n",
    "pathology_detector = CheXagentVisionTransformerPathologyDetector(pathologies=Pathologies.CHEXPERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image 0\n",
      "image_path='test/patient64741/study1/view1_frontal.jpg'\n",
      "Findings:\n",
      "\n",
      "The chest X-ray examination reveals the following abnormalities:\n",
      "\n",
      "* Cardiomegaly is moderately likely, with a detection score of 0.59.\n",
      "* Lung Opacity is highly likely, with a detection score of 0.86, suggesting the presence of a significant amount of abnormal lung tissue.\n",
      "* Atelectasis is moderately likely, with a detection score of 0.58, indicating possible collapse or consolidation of lung tissue.\n",
      "* Pleural Effusion is highly likely, with a detection score of 0.96, suggesting the presence of fluid in the pleural space.\n",
      "* Support Devices are highly likely, with a detection score of 0.93, indicating the presence of external devices such as oxygen tubing or central lines.\n",
      "\n",
      "Overall, the findings suggest the presence of significant lung pathology, including lung opacity and pleural effusion, with possible atelectasis and cardiomegaly. Further evaluation and imaging studies may be necessary to fully characterize these findings and determine the underlying cause.\n"
     ]
    }
   ],
   "source": [
    "chexpert_test_csv_path = Path(\"/vol/biodata/data/chest_xray/CheXpert-v1.0-small/CheXpert-v1.0-small/test.csv\")\n",
    "chexpert_test_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/CheXpert/small/\")\n",
    "\n",
    "with open(chexpert_test_csv_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0].split(\",\")[1:]\n",
    "    # print(header)\n",
    "    for i, line in enumerate(lines[1:]):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Collecting image {i}\")\n",
    "\n",
    "        image_path = line.split(\",\")[0]\n",
    "        print(f\"{image_path=}\")\n",
    "        image_path = chexpert_test_path / image_path\n",
    "\n",
    "        l3.generate_report(image_path, prompt = None, pathology_detector=pathology_detector)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
