{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.hf_cache' ## THIS HAS TO BE BEFORE YOU IMPORT TRANSFORMERS\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from my_secrets import LLAMA3_INSTRUCT_ACCESS_TOKEN, GEMINI_ACCESS_TOKEN\n",
    "from agent_utils import select_best_gpu\n",
    "\n",
    "from pathology_detector import PathologyDetector, CheXagentVisionTransformerPathologyDetector\n",
    "from pathology_sets import Pathologies\n",
    "\n",
    "from phrase_grounder import PhraseGrounder, BioVilTPhraseGrounder\n",
    "\n",
    "\n",
    "class GenerationEngine(ABC):\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str] = None, image_path = None):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def detect_and_localise_pathologies(image_path,pathology_detector,phrase_grounder,pathology_detection_threshold = 0.3, ignore_pathologies = None, do_not_localise = None):\n",
    "\n",
    "        pathology_confidences = pathology_detector.detect_pathologies(image_path, threshold = pathology_detection_threshold)    \n",
    "\n",
    "        pathology_confidences = {pathology: confidence for pathology, confidence in pathology_confidences.items() if pathology not in ignore_pathologies}\n",
    "        pathologies_to_localise = [pathology for pathology in pathology_confidences.keys() if pathology not in do_not_localise]\n",
    "\n",
    "        localised_pathologies = phrase_grounder.get_pathology_lateral_position(image_path, pathologies_to_localise)\n",
    "\n",
    "        return pathology_confidences, localised_pathologies\n",
    "\n",
    "\n",
    "    def generate_prompts(detected_pathologies, localised_pathologies, examples = False):\n",
    "        \n",
    "        if len(detected_pathologies) == 0:\n",
    "            system_prompt = \"\"\" You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. However there is insufficient data to make any comments on pathologies. Just mention it is possible there are no findings and this should be double checked by a radiologist.\"\"\"\n",
    "            image_context_prompt = f\"\"\"No pathologies were detected in the chest X-ray. The user will now interact with you.\"\"\"\n",
    "            return system_prompt, image_context_prompt\n",
    "\n",
    "        if len(localised_pathologies) == 0:\n",
    "            localised_pathologies = \"No lateral positions could be confidently determined for any pathologies detected.\"\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer CONCISELY and professionally as a radiologist would. You should not be confident unless the data is confident. Use language that reflects the confidence of the data.\"\"\"\n",
    "        image_context_prompt = f\"\"\"\n",
    "        You are being provided with data derived from analyzing a chest X-ray, which includes findings on potential pathologies alongside their confidence levels and, separately, possible lateral locations of these pathologies with their own confidence scores.\n",
    "        This information comes from two specialized diagnostic tools.\n",
    "        It's important to recognize how to interpret these datasets together when responding to queries:\n",
    "\n",
    "        Pathology Detection with Confidence Scores:\n",
    "        {detected_pathologies}\n",
    "\n",
    "        Phrase Grounding Locations with Confidence Scores:\n",
    "        {localised_pathologies}\n",
    "\n",
    "        This separate dataset provides potential lateral locations for some of the detected pathologies, each with its own confidence score, indicating the model's certainty about each pathology's location.\n",
    "        For instance, left Pleural Effusion is listed with a confidence score of 0.53, suggesting the location of Pleural Effusion to be on the left side with moderate confidence.\n",
    "\n",
    "        When you interact with end users, remember:\n",
    "\n",
    "        - A pathology and its lateral location (e.g., Pleural Effusion and left Pleural Effusion) are part of the same finding. The location attribute is an additional detail about where the pathology is likely found, not an indicator of a separate pathology. \n",
    "        - Synthesize the pathology detection and localization data. DO NOT TALK ABOUT THEM SEPERATELY. {\"Here is a model example, 'Highly likely there is Pleural Effusion (detection confidence: 0.80), and it is possibly on the left side (localisation confidence: 0.53).'\" if examples else \"\"}\n",
    "        - Confidence scores from the pathology detection and phrase grounding tools are not directly comparable. They serve as indicators of confidence within their respective contexts of pathology detection and localisation.\n",
    "        - A missing lateral location does not imply the absence of a pathology; it indicates the localisation could not be confidently determined.\n",
    "        - If there is any discrepancy between the pathology detection and phrase grounding tools, detection data takes precedence as it more reliably identifies pathologies.\n",
    "\n",
    "        It is important to factor medical knowledge and the specifics of each case, if supplied, into your responses. For example, pathologies located on both sides are called bilateral. Heart related observations are usually on the left/ middle.\n",
    "        This understanding is crucial for accurately processing and responding to queries on the chest X-ray analysis. Structure your answers based on confidence and pathologies.\n",
    "        Double check before you submit your response to ensure you have factored in all the data and followed my instructions carefully.\n",
    "        You will now interact with the user.\n",
    "        \"\"\"\n",
    "            \n",
    "        return system_prompt, image_context_prompt\n",
    "\n",
    "class Llama3Generation(GenerationEngine):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map= select_best_gpu() ,\n",
    "            token=LLAMA3_INSTRUCT_ACCESS_TOKEN,\n",
    "        )\n",
    "\n",
    "    def generate_prompts(detected_pathologies, localised_pathologies, examples = False):\n",
    "\n",
    "        if len(detected_pathologies) == 0:\n",
    "            system_prompt = \"\"\" You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. However there is insufficient data to make any comments on pathologies. Just mention it is possible there are no findings and this should be double checked by a radiologist.\"\"\"\n",
    "            image_context_prompt = f\"\"\"No pathologies were detected in the chest X-ray. The user will now interact with you.\"\"\"\n",
    "            return system_prompt, image_context_prompt\n",
    "\n",
    "        print(localised_pathologies)\n",
    "        if len(localised_pathologies) == 0:\n",
    "            localised_pathologies = \"No lateral positions could be confidently determined for any pathologies detected.\"\n",
    "\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer CONCISELY and professionally as a radiologist would.\"\"\"\n",
    "\n",
    "        image_context_prompt = f\"\"\"\n",
    "        You are given data on a chest x-ray, which includes pathologies and their confidence scores and, separately, possible lateral locations of these pathologies with their own confidence scores.\n",
    "        This information comes from two different, specialized diagnostic tools.\n",
    "        It's important to recognize how to interpret these datasets together when responding to queries:\n",
    "\n",
    "        Pathology Detection with Confidence Scores:\n",
    "        {detected_pathologies}\n",
    "\n",
    "        Here are the guidelines to follow when interpreting the Pathology Detection data - do not mention the confidence scores to the user:\n",
    "\n",
    "        - For confidence scores between 0.3 and 0.5, state \"cannot exclude <pathology>\"\n",
    "        - For confidence scores between 0.5 and 0.7, state \"possible <pathology>\" \n",
    "        - For confidence scores between 0.7 and 0.9, state \"probable <pathology>\"\n",
    "        - For confidence scores over 0.9, simply state the pathology name\n",
    "\n",
    "        Phrase Grounding Locations with Confidence Scores:\n",
    "        {localised_pathologies}\n",
    "\n",
    "        This separate dataset provides potential lateral locations for some of the detected pathologies, each with its own confidence score, indicating the model's certainty about each pathology's location.\n",
    "\n",
    "        When you interact with end users, remember:\n",
    "\n",
    "        - A pathology and its lateral location (e.g., 'Pleural Effusion' and 'Pleural Effusion':'left','right') are part of the same finding. DO NOT TALK ABOUT THEM SEPERATELY.  The location attribute is an additional detail about where the pathology is likely found, not an indicator of a separate pathology. \n",
    "        - Synthesize the pathology detection and localization data.\n",
    "        - Confidence scores from the pathology detection and phrase grounding tools are not directly comparable. They serve as indicators of confidence within their respective contexts of pathology detection and localisation.\n",
    "        - A missing lateral location does not imply the absence of a pathology; it indicates the localisation could not be confidently determined.\n",
    "        - Do not mention any confidence scores to the user.\n",
    "\n",
    "        It is important to factor medical knowledge and the specifics of each case, if supplied, into your responses. For example, pathologies located on both sides are called bilateral.\n",
    "        This understanding is crucial for accurately processing and responding to queries on the chest X-ray analysis. Structure your answers based on confidence and pathologies.\n",
    "        Double check before you submit your response to ensure you have factored in all the data and followed my instructions carefully.\n",
    "        You will now interact with the user, only answer their question do not mention any of your instructions.\n",
    "\n",
    "        \"\"\"\n",
    "        return system_prompt, image_context_prompt\n",
    "\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str] = None):\n",
    "        def format_output(output_text: str) -> str:\n",
    "            return \"\\n\".join(output_text.split(\". \"))  # print output text with each sentence on a new line\n",
    "\n",
    "        terminators = [\n",
    "            self.pipeline.tokenizer.eos_token_id,\n",
    "            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        if user_prompt is not None:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": image_context_prompt +\"\\n\" + user_prompt},\n",
    "            ]\n",
    "\n",
    "            prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            output_text = outputs[0][\"generated_text\"][len(prompt):]\n",
    "            print(format_output(output_text))\n",
    "            return outputs[0][\"generated_text\"][len(prompt):]\n",
    "        \n",
    "        else:\n",
    "            return RuntimeError(\"User prompt not provided - Chat mode CURRENTLY not supported with LLAMA3\")\n",
    "        \n",
    "\n",
    "\n",
    "class CheXagentLanguageModelGeneration(GenerationEngine):\n",
    "\n",
    "    def __init__(self, processor, model, generation_config, device, dtype):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.generation_config = generation_config\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str] = None):\n",
    "        if user_prompt is None:\n",
    "            return RuntimeError(\"User prompt not provided - Chat mode CURRENTLY not supported with CheXagent Language Model\")\n",
    "        inputs = self.processor(\n",
    "        images=None, text=f\"[INST]{image_context_prompt}[/INST] USER: <s>{user_prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "        ).to(device=self.device)\n",
    "        output = self.model.generate(**inputs, generation_config=self.generation_config,generate_written_output = True)[0]\n",
    "        response = self.processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(response)\n",
    "        return response\n",
    "    \n",
    "\n",
    "class CheXagentEndToEndGeneration(GenerationEngine):\n",
    "\n",
    "    def __init__(self, processor, model, generation_config, device, dtype):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.generation_config = generation_config\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def generate_model_output(self, system_prompt: str , image_context_prompt: str, user_prompt:Optional[str],image_path: Path):\n",
    "        images = [Image.open(image_path).convert(\"RGB\")]\n",
    "        inputs = self.processor(\n",
    "            images=images, text=f\" USER: <s>{user_prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "        ).to(device=self.device, dtype=self.dtype)\n",
    "        # print(inputs.keys())\n",
    "        output = self.model.generate(**inputs, generation_config=self.generation_config)[0]\n",
    "        response = self.processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(response)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4090, Free memory: 24177 MB\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Free memory: 24203 MB\n",
      "Selecting GPU 1 with 24203 MB free memory, Device = cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8d8a0f998144f19f7606b784674c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagent Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CXRBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type cxr-bert. This is not supported for all configurations of models and can yield errors.\n",
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at microsoft/BiomedVLP-BioViL-T were not used when initializing CXRBertModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CXRBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CXRBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "GPU 0: NVIDIA GeForce RTX 4090, Free memory: 23788 MB\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Free memory: 6991 MB\n",
      "Selecting GPU 0 with 23788 MB free memory, Device = cuda:0\n",
      "GPU 0: NVIDIA GeForce RTX 4090, Free memory: 23216 MB\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Free memory: 6991 MB\n",
      "Selecting GPU 0 with 23216 MB free memory, Device = cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25cab3eab0e4bfb9ce6d5c9262fa783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cffee5c9c8d4948995861de76893d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pathology_detector = CheXagentVisionTransformerPathologyDetector(pathologies=Pathologies.CHEXPERT)\n",
    "phrase_grounder = BioVilTPhraseGrounder(detection_threshold=0.2)\n",
    "l3 = Llama3Generation()\n",
    "cheXagent_lm = CheXagentLanguageModelGeneration(pathology_detector.processor, pathology_detector.model, pathology_detector.generation_config, pathology_detector.device, pathology_detector.dtype)\n",
    "cheXagent_e2e = CheXagentEndToEndGeneration(pathology_detector.processor, pathology_detector.model, pathology_detector.generation_config, pathology_detector.device, pathology_detector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3 Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_l3_prompts(detected_pathologies, localised_pathologies, examples = True):\n",
    "\n",
    "    if len(detected_pathologies) == 0:\n",
    "        system_prompt = \"\"\" You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. However there is insufficient data to make any comments on pathologies. Just mention it is possible there are no findings and this should be double checked by a radiologist.\"\"\"\n",
    "        image_context_prompt = f\"\"\"No pathologies were detected in the chest X-ray. The user will now interact with you.\"\"\"\n",
    "        return system_prompt, image_context_prompt\n",
    "\n",
    "    print(localised_pathologies)\n",
    "    if len(localised_pathologies) == 0:\n",
    "        localised_pathologies = \"No lateral positions could be confidently determined for any pathologies detected.\"\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer CONCISELY and professionally as a radiologist would. Do not reference any confidence scores in your responses.\"\"\"\n",
    "\n",
    "    image_context_prompt = f\"\"\"\n",
    "    You are given data on a chest x-ray, which includes pathologies and their confidence scores and, separately, possible lateral locations of these pathologies with their own confidence scores.\n",
    "    This information comes from two different, specialized diagnostic tools.\n",
    "    It's important to recognize how to interpret these datasets together when responding to queries:\n",
    "\n",
    "    Pathology Detection with Confidence Scores:\n",
    "    {detected_pathologies}\n",
    "\n",
    "    Here are the guidelines to follow when interpreting the Pathology Detection data - do not mention the confidence scores to the user:\n",
    "\n",
    "    - For confidence scores between 0.3 and 0.5, state \"cannot exclude <pathology>\"\n",
    "    - For confidence scores between 0.5 and 0.7, state \"possible <pathology>\" \n",
    "    - For confidence scores between 0.7 and 0.9, state \"probable <pathology>\"\n",
    "    - For confidence scores over 0.9, simply state the pathology name\n",
    "\n",
    "    Phrase Grounding Locations with Confidence Scores:\n",
    "    {localised_pathologies}\n",
    "\n",
    "    This separate dataset provides potential lateral locations for some of the detected pathologies, each with its own confidence score, indicating the model's certainty about each pathology's location.\n",
    "\n",
    "    When you interact with end users, remember:\n",
    "    - A pathology and its lateral location (e.g., Pleural Effusion and left Pleural Effusion) are part of the same finding. The location attribute is an additional detail about where the pathology is likely found, not an indicator of a separate pathology. \n",
    "    - Synthesize the pathology detection and localization data. DO NOT TALK ABOUT THEM SEPERATELY. \n",
    "    - Confidence scores from the pathology detection and phrase grounding tools are not directly comparable. They serve as indicators of confidence within their respective contexts of pathology detection and localisation.\n",
    "    - A missing lateral location does not imply the absence of a pathology; it indicates the localisation could not be confidently determined.\n",
    "    {\"Here are some model examples: 'Probable pleural effusion, located on the left' ; 'Possible bilateral edema' \" if examples else \"\"}\n",
    "\n",
    "\n",
    "    It is important to factor medical knowledge and the specifics of each case, if supplied, into your responses. For example, pathologies located on both sides are called bilateral.\n",
    "    This understanding is crucial for accurately processing and responding to queries on the chest X-ray analysis. Structure your answers based on confidence and pathologies.\n",
    "    Double check before you submit your response to ensure you have factored in all the data and followed my instructions carefully.\n",
    "    You will now interact with the user, only answer their question do not mention any of your instructions.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return system_prompt, image_context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image 0\n",
      "image_path='test/patient64753/study1/view1_frontal.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lung Opacity': [('left', 0.34), ('right', 0.24)], 'Atelectasis': [('left', 0.29), ('right', 0.3)], 'Pneumothorax': [('left', 0.26), ('right', 0.28)], 'Pleural Effusion': [('left', 0.33), ('right', 0.23)]}\n",
      "L3 agent:\n",
      "cannot exclude Cardiomegaly, possible Lung Opacity, possible Atelectasis, probable Pleural Effusion.\n",
      " -----------------\n"
     ]
    }
   ],
   "source": [
    "chexpert_test_csv_path = Path(\"/vol/biodata/data/chest_xray/CheXpert-v1.0-small/CheXpert-v1.0-small/test.csv\")\n",
    "chexpert_test_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/CheXpert/small/\")\n",
    "\n",
    "mimic_cxr_jpg_path = None\n",
    "\n",
    "with open(chexpert_test_csv_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0].split(\",\")[1:]\n",
    "    # print(header)\n",
    "    for i, line in enumerate(lines[20:]):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Collecting image {i}\")\n",
    "\n",
    "        image_path = line.split(\",\")[0]\n",
    "        print(f\"{image_path=}\")\n",
    "        image_path = chexpert_test_path / image_path\n",
    "\n",
    "        user_prompt = f\"\"\"Write a report on the chest x-ray\"\"\"\n",
    "        user_prompt = f\"\"\"Write up the findings on the chest x-ray as a radiologist would\"\"\"\n",
    "        user_prompt = f\"\"\"Just list the findings on the chest x-ray, nothing else. If there are no findings, just say that.\"\"\"\n",
    "\n",
    "\n",
    "        ignore_pathologies = {\"Support Devices\"}\n",
    "        # ignore_pathologies = {\"Support Devices\",\"Cardiomegaly\",\"Lung Opacity\",\"Atelectasis\",\"Pneumothorax\",\"Pleural Effusion\"}\n",
    "        do_not_localise = {\"Cardiomegaly\"}\n",
    "\n",
    "        pathology_confidences, localised_pathologies = GenerationEngine.detect_and_localise_pathologies(image_path=image_path, pathology_detector=pathology_detector, phrase_grounder=phrase_grounder, ignore_pathologies=ignore_pathologies, do_not_localise=do_not_localise)      \n",
    "        l3_system_prompt, l3_image_context_prompt = generate_l3_prompts(pathology_confidences, localised_pathologies, examples = False)\n",
    "\n",
    "        # print(\"System Prompt: \\n\")\n",
    "        # print(f\"{l3_system_prompt}\\n\")\n",
    "        # print(\"Image Context Prompt:\\n\")\n",
    "        # print(f\"{l3_image_context_prompt}\\n\")\n",
    "        \n",
    "        print(\"L3 agent:\") \n",
    "        l3.generate_model_output(l3_system_prompt, l3_image_context_prompt, user_prompt=user_prompt)\n",
    "        \n",
    "        # system_prompt, image_context_prompt = GenerationEngine.contextualise_model(image_path=image_path,pathology_detector=pathology_detector, phrase_grounder=phrase_grounder, examples=False)\n",
    "        # print(\"CheXagent agent:\")\n",
    "        # cheXagent_lm.generate_model_output(system_prompt, image_context_prompt,user_prompt=user_prompt)\n",
    "        \n",
    "        # print(\"CheXagent:\")\n",
    "        # cheXagent_e2e.generate_model_output(system_prompt, image_context_prompt,user_prompt=user_prompt, image_path=image_path)\n",
    "        print(f\" -----------------\")\n",
    "        if i == 0: \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GEMINI 1.5 Flash Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_prompts(detected_pathologies, localised_pathologies, examples = True):\n",
    "\n",
    "    if len(detected_pathologies) == 0:\n",
    "        system_prompt = \"\"\" You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. However there is insufficient data to make any comments on pathologies. Just mention it is possible there are no findings and this should be double checked by a radiologist.\"\"\"\n",
    "        image_context_prompt = f\"\"\"No pathologies were detected in the chest X-ray. The user will now interact with you.\"\"\"\n",
    "        return system_prompt, image_context_prompt\n",
    "\n",
    "    print(localised_pathologies)\n",
    "    if len(localised_pathologies) == 0:\n",
    "        localised_pathologies = \"No lateral positions could be confidently determined for any pathologies detected.\"\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer CONCISELY and professionally as a radiologist would.\"\"\"\n",
    "\n",
    "    image_context_prompt = f\"\"\"\n",
    "    You are given data on a chest x-ray, which includes pathologies and their confidence scores and, separately, possible lateral locations of these pathologies with their own confidence scores.\n",
    "    It's important to recognize how to interpret these datasets together when responding to queries:\n",
    "\n",
    "    Pathology Detection with Confidence Scores:\n",
    "    {detected_pathologies}\n",
    "\n",
    "    Here are the guidelines to follow when interpreting the Pathology Detection data - do not mention the confidence scores to the user:\n",
    "\n",
    "    - For confidence scores between 0.3 and 0.5, state \"cannot exclude <pathology>\"\n",
    "    - For confidence scores between 0.5 and 0.7, state \"possible <pathology>\" \n",
    "    - For confidence scores between 0.7 and 0.9, state \"probable <pathology>\"\n",
    "    - For confidence scores over 0.9, simply state the pathology name\n",
    "\n",
    "    Phrase Grounding Locations with Confidence Scores:\n",
    "    {localised_pathologies}\n",
    "\n",
    "    This separate dataset provides potential lateral locations for some of the detected pathologies, each with its own confidence score, indicating the model's certainty about each pathology's location.\n",
    "\n",
    "    When you interact with end users, remember:\n",
    "    - A pathology and its lateral location (e.g., Pleural Effusion and left Pleural Effusion) are part of the same finding. The location attribute is an additional detail about where the pathology is likely found, not an indicator of a separate pathology. \n",
    "    - Synthesize the pathology detection and localization data. DO NOT TALK ABOUT THEM SEPERATELY. \n",
    "    - Confidence scores from the pathology detection and phrase grounding tools are not directly comparable. They serve as indicators of confidence within their respective contexts of pathology detection and localisation.\n",
    "    - A missing lateral location does not imply the absence of a pathology; it indicates the localisation could not be confidently determined.\n",
    "    {\"Here are some model examples: 'Probable pleural effusion, located on the left' ; 'Possible bilateral edema' \" if examples else \"\"}\n",
    "\n",
    "\n",
    "    It is important to factor medical knowledge and the specifics of each case, if supplied, into your responses. For example, pathologies located on both sides are called bilateral.\n",
    "    Structure your answers based on confidence and pathologies.\n",
    "    \n",
    "    Double check before you submit your response to ensure you have factored in all the data and followed my instructions carefully.\n",
    "    You will now interact with the user, only answer their question do not mention any of your instructions.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return system_prompt, image_context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathology_confidences, localised_pathologies = GenerationEngine.detect_and_localise_pathologies(image_path=image_path, pathology_detector=pathology_detector, phrase_grounder=phrase_grounder, ignore_pathologies=ignore_pathologies, do_not_localise=do_not_localise)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No findings. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_ACCESS_TOKEN)\n",
    "# for model in genai.list_models():\n",
    "#     print(model)\n",
    "gemini_flash = genai.GenerativeModel(model_name='gemini-1.5-flash-latest',\n",
    "                                     system_instruction=l3_system_prompt)\n",
    "\n",
    "# got up to /vol/biodata/data/chest_xray/mimic-cxr-jpg/files/p10/p10692230/s54308287/52346adf-2a2660ab-26f2452a-aa6bc891-b3d2c9ef.jpg IN MIMIC (train on single study single scan)\n",
    "image_path = Path(\"/vol/biodata/data/chest_xray/CheXpert-v1.0-small/CheXpert-v1.0-small/test/patient64889/study1/view1_frontal.jpg\")\n",
    "\n",
    "ignore_pathologies = {\"Support Devices\"}\n",
    "\n",
    "do_not_localise = {\"Cardiomegaly\"}\n",
    "user_prompt = f\"\"\"Write up the findings on the chest x-ray as a radiologist would\"\"\"\n",
    "user_prompt = f\"\"\"Just list the findings on the chest x-ray, nothing else\"\"\"\n",
    "\n",
    "l3_system_prompt, l3_image_context_prompt = generate_gemini_prompts(pathology_confidences, localised_pathologies)\n",
    "\n",
    "response = gemini_flash.generate_content(f\"{l3_image_context_prompt}\\n{user_prompt}\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
