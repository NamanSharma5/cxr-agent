{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.hf_cache' ## THIS HAS TO BE BEFORE YOU IMPORT TRANSFORMERS\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from my_secrets import LLAMA3_INSTRUCT_ACCESS_TOKEN\n",
    "from agent_utils import select_best_gpu\n",
    "\n",
    "from pathology_detector import PathologyDetector, CheXagentVisionTransformerPathologyDetector\n",
    "from pathology_sets import Pathologies\n",
    "\n",
    "from phrase_grounder import PhraseGrounder, BioVilTPhraseGrounder\n",
    "\n",
    "class GenerationEngine(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_report(self, image_path: Path, prompt: Optional[str], output_dir: Optional[str]) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Llama3Generation(GenerationEngine):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map= select_best_gpu() ,\n",
    "            token=LLAMA3_INSTRUCT_ACCESS_TOKEN,\n",
    "        )\n",
    "\n",
    "\n",
    "    def generate_report(self, image_path: Path, prompt: Optional[str], pathology_detector: PathologyDetector, phrase_grounder: Optional[PhraseGrounder]) -> str:\n",
    "        \n",
    "        if pathology_detector is not None:\n",
    "            pathology_confidences = pathology_detector.detect_pathologies(image_path, threshold = 0.5)\n",
    "        else:\n",
    "            return RuntimeError(\"Pathology detector not provided\")\n",
    "        \n",
    "        \n",
    "\n",
    "        #### PROMPT PIPELINE ###\n",
    "        \n",
    "        system_prompt = \"\"\"You are a helpful assistant, specialising in radiology and interpreting Chest X-rays. Please answer concisely and in a professional manner.\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Using specialised pathology detection tools,\n",
    "        you are given the following pathology detection results for a chest X-ray:\n",
    "        {pathology_confidences}\n",
    "\n",
    "        Please note the closer the value to 1, the more likely the pathology is present in the image. \n",
    "        Write up a findings section based on these observations\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt },\n",
    "        ]\n",
    "\n",
    "        prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        terminators = [\n",
    "            self.pipeline.tokenizer.eos_token_id,\n",
    "            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        outputs = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "        return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CXRBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type cxr-bert. This is not supported for all configurations of models and can yield errors.\n",
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at microsoft/BiomedVLP-BioViL-T were not used when initializing CXRBertModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CXRBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CXRBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "GPU 0: NVIDIA TITAN RTX, Free memory: 22965 MB\n",
      "GPU 1: NVIDIA TITAN RTX, Free memory: 24109 MB\n",
      "Selecting GPU 1 with 24109 MB free memory, Device = cuda:1\n"
     ]
    }
   ],
   "source": [
    "# l3 = Llama3Generation()\n",
    "# pathology_detector = CheXagentVisionTransformerPathologyDetector(pathologies=Pathologies.CHEXPERT)\n",
    "phrase_grounder = BioVilTPhraseGrounder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left Aortic enlargement': 0.2655527722760835, 'right Aortic enlargement': 0.2631697580533245}\n"
     ]
    }
   ],
   "source": [
    "vindr_image_id = \"7a74f4e463b72f018838f26cf7aabdf2\"\n",
    "vindr_png_path = Path('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test')\n",
    "image_path = vindr_png_path / f\"{vindr_image_id}.png\"\n",
    "\n",
    "phrase_grounder_output = phrase_grounder.get_pathology_lateral_position(image_path, \"Aortic enlargement\")\n",
    "print(phrase_grounder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image 0\n",
      "image_path='test/patient64741/study1/view1_frontal.jpg'\n",
      "Findings:\n",
      "\n",
      "The chest X-ray examination reveals the following abnormalities:\n",
      "\n",
      "* Cardiomegaly is moderately likely, with a detection score of 0.59.\n",
      "* Lung Opacity is highly likely, with a detection score of 0.86, suggesting the presence of a significant amount of abnormal lung tissue.\n",
      "* Atelectasis is moderately likely, with a detection score of 0.58, indicating possible collapse or consolidation of lung tissue.\n",
      "* Pleural Effusion is highly likely, with a detection score of 0.96, suggesting the presence of fluid in the pleural space.\n",
      "* Support Devices are highly likely, with a detection score of 0.93, indicating the presence of external devices such as oxygen tubing or central lines.\n",
      "\n",
      "Overall, the findings suggest the presence of significant lung pathology, including lung opacity and pleural effusion, with possible atelectasis and cardiomegaly. Further evaluation and imaging studies may be necessary to fully characterize these findings and determine the underlying cause.\n"
     ]
    }
   ],
   "source": [
    "chexpert_test_csv_path = Path(\"/vol/biodata/data/chest_xray/CheXpert-v1.0-small/CheXpert-v1.0-small/test.csv\")\n",
    "chexpert_test_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/CheXpert/small/\")\n",
    "\n",
    "with open(chexpert_test_csv_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0].split(\",\")[1:]\n",
    "    # print(header)\n",
    "    for i, line in enumerate(lines[1:]):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Collecting image {i}\")\n",
    "\n",
    "        image_path = line.split(\",\")[0]\n",
    "        print(f\"{image_path=}\")\n",
    "        image_path = chexpert_test_path / image_path\n",
    "\n",
    "        l3.generate_report(image_path, prompt = None, pathology_detector=pathology_detector)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
