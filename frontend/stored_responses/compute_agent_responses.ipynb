{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sys \n",
    "# Add the base_agent directory to sys.path\n",
    "base_agent_dir = '/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/base_agent'\n",
    "sys.path.insert(0, str(base_agent_dir))\n",
    "\n",
    "from pathology_sets import Pathologies\n",
    "from pathology_detector import CheXagentVisionTransformerPathologyDetector\n",
    "from phrase_grounder import BioVilTPhraseGrounder\n",
    "from generation_engine import GenerationEngine, Llama3Generation, CheXagentLanguageModelGeneration, GeminiFlashGeneration\n",
    "# USER_PROMPT = \"Generate the findings section of the radiology report for the scan\"\n",
    "USER_PROMPT = \"What are the findings?\"\n",
    "USER_PROMPT = \"Generate the findings section ONLY of the radiology report for this scan\"\n",
    "PATHOLOGY_DETECTION_THRESHOLD = 0.4\n",
    "PHRASE_GROUNDING_THRESHOLD = 0.2\n",
    "IGNORE_PATHOLOGIES = {\"Support Devices\"}\n",
    "DO_NOT_LOCALISE = {\"Cardiomegaly\"}\n",
    "CHEXPERT = True\n",
    "FILE_NAME = \"findings_section_of_report\"\n",
    "SHUFFLE = False\n",
    "DEVICE = None #\"cuda:1\"   \n",
    "\n",
    "IMAGE_DIR_PREFIX = Path(\"/vol/biodata/data/chest_xray/mimic-cxr-jpg/files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX 6000 Ada Generation, Free memory: 48643 MB\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation, Free memory: 41465 MB\n",
      "Selecting GPU 0 with 48643 MB free memory, Device = cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049dab5f61474da2a776821c4aba6081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CXRBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagent Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type cxr-bert. This is not supported for all configurations of models and can yield errors.\n",
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at microsoft/BiomedVLP-BioViL-T were not used when initializing CXRBertModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CXRBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CXRBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation, Free memory: 31391 MB\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation, Free memory: 41465 MB\n",
      "Selecting GPU 1 with 41465 MB free memory, Device = cuda:1\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation, Free memory: 31391 MB\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation, Free memory: 40463 MB\n",
      "Selecting GPU 1 with 40463 MB free memory, Device = cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b16fea245944ae93af602d85911d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pathology_detector = CheXagentVisionTransformerPathologyDetector(pathologies=Pathologies.CHEXPERT, device=DEVICE)\n",
    "phrase_grounder = BioVilTPhraseGrounder(detection_threshold=PHRASE_GROUNDING_THRESHOLD, device = DEVICE)\n",
    "l3 = Llama3Generation(device = DEVICE)\n",
    "cheXagent_lm = CheXagentLanguageModelGeneration(pathology_detector.processor, pathology_detector.model, pathology_detector.generation_config, pathology_detector.device, pathology_detector.dtype)\n",
    "gemini = GeminiFlashGeneration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_to_image_path = {}\n",
    "# Read the data file into a list of dictionaries\n",
    "def read_data_file(cheXpert = CHEXPERT):\n",
    "\n",
    "    subjects = []\n",
    "    if cheXpert:\n",
    "        cheXpert_small_test_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/CheXpert/small/test\")\n",
    "        cheXpert_test_path = Path(\"/vol/biodata/data/chest_xray/CheXpert-v1.0-small/CheXpert-v1.0-small/test\")\n",
    "\n",
    "        cheXpert_subjects = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/evaluation_datasets/CheXpert/chexpert_test_random_written_pathologies\")\n",
    "\n",
    "        for line in cheXpert_subjects.read_text().splitlines():\n",
    "            subject = line.split(\",\")[0]\n",
    "            subjects.append(subject)\n",
    "            subject_path_jpg = cheXpert_test_path / subject\n",
    "            subject_to_image_path[subject] = subject_path_jpg      \n",
    "    \n",
    "    else:\n",
    "        mimic_cxr_jpg_path = Path('/vol/biodata/data/chest_xray/mimic-cxr-jpg')\n",
    "        # subject_with_single_scan_no_prior_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/frontend/subjects_with_single_scan_no_prior.txt\")\n",
    "        subjects_for_eval = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/evaluation_datasets/MIMIC-CXR/mimic_ii_subjects_for_eval.txt\")\n",
    "        for line in subjects_for_eval.read_text().splitlines():\n",
    "            # two cases: 1) subject with single study 2) subject with multiple studies\n",
    "            if line[0] == \"p\":\n",
    "                parts = line.split(\"/\")\n",
    "                subject = parts[0][1:] # remove the 'p' from the subject\n",
    "                subjects.append(subject) \n",
    "                study = parts[1][1:]\n",
    "\n",
    "                subject_path_jpg = mimic_cxr_jpg_path/\"files\"/f\"p{subject[:2]}\"/f\"p{subject}\"/f\"s{study}\"\n",
    "                jpg_file = list(subject_path_jpg.glob('*.jpg'))[0]\n",
    "\n",
    "            else:\n",
    "                subject = line\n",
    "                subjects.append(subject)\n",
    "                subject_path_jpg = mimic_cxr_jpg_path/\"files\"/ f\"p{subject[:2]}/p{subject}\"\n",
    "                \n",
    "                study_folder = list(subject_path_jpg.glob('*'))[0]\n",
    "                jpg_file = list(study_folder.glob('*.jpg'))[0]\n",
    "                \n",
    "            subject_to_image_path[subject] = jpg_file\n",
    "            \n",
    "    return subjects\n",
    "\n",
    "subjects = read_data_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.hf_cache/modules/transformers_modules/4934e91451945c8218c267aae9c34929a7677829/processing_chexagent.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(pixel_values) for pixel_values in encoding_image_processor[\"pixel_values\"]]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "image_path_to_model_responses = defaultdict(dict)\n",
    "\n",
    "for subject in subjects:\n",
    "    image_path = subject_to_image_path[subject]\n",
    "\n",
    "    pathology_confidences, localised_pathologies, chexagent_e2e = GenerationEngine.detect_and_localise_pathologies(\n",
    "        image_path=image_path,\n",
    "        pathology_detector=pathology_detector,\n",
    "        phrase_grounder=phrase_grounder,\n",
    "        pathology_detection_threshold = PATHOLOGY_DETECTION_THRESHOLD,\n",
    "        ignore_pathologies=IGNORE_PATHOLOGIES,\n",
    "        do_not_localise=DO_NOT_LOCALISE,\n",
    "        prompt_for_chexagent_lm_output= USER_PROMPT,\n",
    "    )\n",
    "\n",
    "    model_outputs = {}\n",
    "    model_outputs['chexagent'] = chexagent_e2e\n",
    "\n",
    "    gemini_system_prompt, gemini_image_context_prompt = gemini.generate_prompts(pathology_confidences, localised_pathologies)\n",
    "    model_outputs['gemini_agent'] = gemini.generate_model_output(gemini_system_prompt, gemini_image_context_prompt, user_prompt=USER_PROMPT)\n",
    "\n",
    "    system_prompt, image_context_prompt = GenerationEngine.generate_prompts(pathology_confidences, localised_pathologies)\n",
    "    model_outputs['chexagent_agent'] = cheXagent_lm.generate_model_output(system_prompt, image_context_prompt, user_prompt=USER_PROMPT)\n",
    "\n",
    "    system_prompt, image_context_prompt = l3.generate_prompts(pathology_confidences, localised_pathologies)\n",
    "    model_outputs['llama3_agent'] = l3.generate_model_output(system_prompt, image_context_prompt, user_prompt=USER_PROMPT)\n",
    "\n",
    "    image_path_to_model_responses[image_path] = model_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image_path_to_model_responses as a pickle file\n",
    "if CHEXPERT:\n",
    "    output_file = Path(f\"/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/frontend/stored_responses/CheXpert/{FILE_NAME}.pkl\")\n",
    "else:\n",
    "    output_file = Path(f\"/vol/biomedic3/bglocker/ugproj2324/nns20/cxr-agent/frontend/stored_responses/MIMIC-CXR/{FILE_NAME}.pkl\")\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(image_path_to_model_responses, f)\n",
    "\n",
    "# # read the pickle file\n",
    "# with open(output_file, 'rb') as f:\n",
    "#     image_path_to_model_responses = pickle.load(f)\n",
    "#     print(image_path_to_model_responses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
