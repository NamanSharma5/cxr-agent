{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -------------------------------------------------------------------------------------------\n",
    "#  Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n",
    "#  -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase grounding\n",
    "\n",
    "This notebook demonstrates the usage of the BioViL-T image and text models in a multimodal phrase grounding setting.\n",
    "Given a chest X-ray and a radiology text phrase, the joint model grounds the phrase in the image, i.e., highlights the regions of the image that share features similar to the phrase.\n",
    "Please refer to [our ECCV and CVPR papers](https://hi-ml.readthedocs.io/en/latest/multimodal.html#credit) for further details.\n",
    "\n",
    "The notebook can also be run on Binder without the need of any coding or local installation:\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/hi-ml/HEAD?labpath=hi-ml-multimodal%2Fnotebooks%2Fphrase_grounding.ipynb)\n",
    "\n",
    "This demo is solely for research evaluation purposes, not intended to be a medical product or clinical use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's first install the `hi-ml-multimodal` Python package, which will allow us to import the `health_multimodal` Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/biomedic3/bglocker/ugproj2324/nns20/venvs/phrase_grounding/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/vol/biomedic3/bglocker/ugproj2324/nns20/venvs/phrase_grounding/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pip_source = \"hi-ml-multimodal\"\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/vol/biomedic3/bglocker/ugproj2324/nns20/.hi-ml-cache\"\n",
    "\n",
    "from health_multimodal.common.visualization import plot_phrase_grounding_similarity_map\n",
    "from health_multimodal.text import get_bert_inference\n",
    "from health_multimodal.text.utils import BertEncoderType\n",
    "from health_multimodal.image import get_image_inference\n",
    "from health_multimodal.image.utils import ImageModelType\n",
    "from health_multimodal.vlp import ImageTextInferenceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multimodal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text and image models from [Hugging Face ðŸ¤—](https://aka.ms/biovil-models) and instantiate the inference engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CXRBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type cxr-bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at microsoft/BiomedVLP-BioViL-T were not used when initializing CXRBertModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CXRBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CXRBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n"
     ]
    }
   ],
   "source": [
    "text_inference = get_bert_inference(BertEncoderType.BIOVIL_T_BERT)\n",
    "image_inference = get_image_inference(ImageModelType.BIOVIL_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the joint inference engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_text_inference = ImageTextInferenceEngine(\n",
    "    image_inference_engine=image_inference,\n",
    "    text_inference_engine=text_inference,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_text_inference.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -------------------------------------------------------------------------------------------\n",
    "#  Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n",
    "#  -------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from PIL import Image\n",
    "\n",
    "from health_multimodal.image.data.io import load_image\n",
    "\n",
    "\n",
    "TypeArrayImage = Union[np.ndarray, Image.Image]\n",
    "\n",
    "\n",
    "def _plot_image(\n",
    "    image: TypeArrayImage,\n",
    "    axis: plt.Axes,\n",
    "    title: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Plot an image on a given axis, deleting the axis ticks and axis labels.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param axis: Axis to plot the image on.\n",
    "    :param title: Title used for the axis.\n",
    "    \"\"\"\n",
    "    axis.imshow(image)\n",
    "    axis.axis(\"off\")\n",
    "    if title is not None:\n",
    "        axis.set_title(title)\n",
    "\n",
    "\n",
    "def _get_isolines_levels(step_size: float) -> np.ndarray:\n",
    "    num_steps = np.floor(round(1 / step_size)).astype(int)\n",
    "    levels = np.linspace(step_size, 1, num_steps)\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _plot_isolines(\n",
    "    image: TypeArrayImage,\n",
    "    heatmap: np.ndarray,\n",
    "    axis: plt.Axes,\n",
    "    title: Optional[str] = None,\n",
    "    colormap: str = \"RdBu_r\",\n",
    "    step: float = 0.25,\n",
    ") -> None:\n",
    "    \"\"\"Plot an image and overlay heatmap isolines on it.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param heatmap: Heatmap of the same size as the image.\n",
    "    :param axis: Axis to plot the image on.\n",
    "    :param title: Title used for the axis.\n",
    "    :param colormap: Name of the Matplotlib colormap used for the isolines.\n",
    "    :param step: Step size between the isolines levels. The levels are in :math:`(0, 1]`.\n",
    "        For example, a step size of 0.25 will result in isolines levels of 0.25, 0.5, 0.75 and 1.\n",
    "    \"\"\"\n",
    "    axis.imshow(image)\n",
    "    levels = _get_isolines_levels(step)\n",
    "    contours = axis.contour(\n",
    "        heatmap,\n",
    "        cmap=colormap,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        levels=levels,\n",
    "    )\n",
    "    axis.clabel(contours, inline=True, fontsize=10)\n",
    "    axis.axis(\"off\")\n",
    "    if title is not None:\n",
    "        axis.set_title(title)\n",
    "\n",
    "\n",
    "def _plot_heatmap(\n",
    "    image: TypeArrayImage,\n",
    "    heatmap: np.ndarray,\n",
    "    figure: plt.Figure,\n",
    "    axis: plt.Axes,\n",
    "    colormap: str = \"RdBu_r\",\n",
    "    title: Optional[str] = None,\n",
    "    alpha: float = 0.5,\n",
    ") -> None:\n",
    "    \"\"\"Plot a heatmap overlaid on an image.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param heatmap: Input heatmap of the same size as the image.\n",
    "    :param figure: Figure to plot the images on.\n",
    "    :param axis: Axis to plot the images on.\n",
    "    :param colormap: Name of the Matplotlib colormap for the heatmap.\n",
    "    :param title: Title used for the axis.\n",
    "    :param alpha: Heatmap opacity. Must be in :math:`[0, 1]`.\n",
    "    \"\"\"\n",
    "    axis.imshow(image)\n",
    "    axes_image = axis.matshow(heatmap, alpha=alpha, cmap=colormap, vmin=-1, vmax=1)\n",
    "    # https://www.geeksforgeeks.org/how-to-change-matplotlib-color-bar-size-in-python/\n",
    "    divider = make_axes_locatable(axis)\n",
    "    colorbar_axes = divider.append_axes(\"right\", size=\"10%\", pad=0.1)\n",
    "    colorbar = figure.colorbar(axes_image, cax=colorbar_axes)\n",
    "    # https://stackoverflow.com/a/50671487/3956024\n",
    "    colorbar.ax.tick_params(pad=35)\n",
    "    plt.setp(colorbar.ax.get_yticklabels(), ha=\"right\")\n",
    "    axis.axis(\"off\")\n",
    "    if title is not None:\n",
    "        axis.set_title(title)\n",
    "\n",
    "\n",
    "def plot_phrase_grounding_similarity_map(\n",
    "    image_path: Path, similarity_map: np.ndarray, bboxes: Optional[List[Tuple[float, float, float, float]]] = None, text_prompt = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Plot visualization of the input image, the similarity heatmap and the heatmap isolines.\n",
    "\n",
    "    :param image_path: Path to the input image.\n",
    "    :param similarity_map: Phase grounding similarity map of the same size as the image.\n",
    "    :param bboxes: Optional list of bounding boxes to plot on the image.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "    image = load_image(image_path).convert(\"RGB\")\n",
    "    _plot_image(image, axis=axes[0], title=\"Input image\")\n",
    "    _plot_isolines(image, similarity_map, axis=axes[1], title=\"Similarity isolines\")\n",
    "    _plot_heatmap(image, similarity_map, figure=fig, axis=axes[2], title=\"Similarity heatmap\")\n",
    "    if bboxes is not None:\n",
    "        _plot_bounding_boxes(ax=axes[1], bboxes=bboxes)\n",
    "    if text_prompt is not None:\n",
    "        # add text prompt to the figure\n",
    "        fig.suptitle(text_prompt, fontsize=16)\n",
    "    \n",
    "    # plot a line down the center of the image\n",
    "    axes[0].axvline(x=image.size[0] / 2, color=\"black\", linestyle=\"--\")\n",
    "    print(\"image.size[0] / 2\", image.size[0] / 2)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _plot_bounding_boxes(\n",
    "    ax: plt.Axes, bboxes: List[Tuple[float, float, float, float]], linewidth: float = 1.5, alpha: float = 0.45\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot bounding boxes on an existing axes object.\n",
    "\n",
    "    :param ax: The axes object to plot the bounding boxes on.\n",
    "    :param bboxes: A list of bounding box coordinates as (x, y, width, height) tuples.\n",
    "    :param linewidth: Optional line width for the bounding box edges (default is 2).\n",
    "    :param alpha: Optional opacity for the bounding box edges (default is 1.0).\n",
    "    \"\"\"\n",
    "    for bbox in bboxes:\n",
    "        x, y, width, height = bbox\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), width, height, linewidth=linewidth, edgecolor='k', facecolor='none', linestyle='--', alpha=alpha\n",
    "        )\n",
    "        ax.add_patch(rect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeBox = Tuple[float, float, float, float]\n",
    "\n",
    "def plot_phrase_grounding(image_path: Path, text_prompt: str, bboxes: List[TypeBox], other_text_prompt = None) -> None:\n",
    "    similarity_map = image_text_inference.get_similarity_map_from_raw_data(\n",
    "        image_path=image_path,\n",
    "        query_text=text_prompt,\n",
    "        interpolation=\"bilinear\",\n",
    "    )\n",
    "\n",
    "    plot_phrase_grounding_similarity_map(\n",
    "        image_path=image_path,\n",
    "        similarity_map=similarity_map,\n",
    "        bboxes=bboxes,\n",
    "        text_prompt = f\"{other_text_prompt} {text_prompt}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold determination\n",
    "\n",
    "Trying to figure out what is a good threshold for image text reasoning based tasks (i.e. at what point can I consider a pathology or a phrase to be present in the image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "top_n = 20\n",
    "\n",
    "def get_top_values(similarity_map):\n",
    "    top_values = []\n",
    "    for i in range(similarity_map.shape[0]):\n",
    "        for j in range(similarity_map.shape[1]):\n",
    "            if similarity_map[i, j] > threshold:\n",
    "                top_values.append((i, j, similarity_map[i, j]))\n",
    "\n",
    "    top_values = sorted(top_values, key = lambda x: x[2], reverse = True)\n",
    "    return top_values[:top_n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_value_average=0.3559772091989896\n",
      "top_5_value_average=0.3557334352675844\n",
      "zero_count=70\n",
      "count=2038\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "test_pathology_left_or_right = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/VinDr-CXR/image_text_reasoning_datasets/test_pathology_left_or_right\")\n",
    "vindr_png_path = Path('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test')\n",
    "\n",
    "top_value_average = 0\n",
    "top_5_value_average = 0\n",
    "count = 0\n",
    "zero_count = 0\n",
    "\n",
    "with open(test_pathology_left_or_right) as f:\n",
    "\n",
    "    for line in f.readlines():\n",
    "        image_id, text_prompt, _ = line.strip().split(\",\")\n",
    "        image_path = vindr_png_path / f\"{image_id}.png\"\n",
    "        # similarity_map = image_text_inference.get_similarity_map_from_raw_data(\n",
    "        # image_path=image_path,\n",
    "        # query_text=text_prompt,\n",
    "        # interpolation=\"bilinear\",\n",
    "        # )\n",
    "        similarity_map = image_text_inference.get_similarity_map_from_raw_data(\n",
    "        image_path=image_path,\n",
    "        query_text=text_prompt,\n",
    "        interpolation=\"bilinear\",\n",
    "        )\n",
    "\n",
    "        # go through all the values in the similarity map and print out the top 5 values\n",
    "        top_values = get_top_values(similarity_map)\n",
    "\n",
    "        if len(top_values) == 0:\n",
    "            zero_count += 1\n",
    "            continue\n",
    "        top_value_average += top_values[0][2]\n",
    "        top_5_value_average += sum([x[2] for x in top_values[:5]])\n",
    "        count += 1\n",
    "        # bboxes = [tuple(map(float, box.split(\",\"))) for box in bboxes.split(\";\")]\n",
    "        # plot_phrase_grounding\n",
    "\n",
    "top_value_average /= count\n",
    "top_5_value_average /= (count*5)\n",
    "\n",
    "print(f\"{top_value_average=}\")\n",
    "print(f\"{top_5_value_average=}\")\n",
    "print(f\"{zero_count=}\")\n",
    "print(f\"{count=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latent perfomance on CheXbench task\n",
    "\n",
    "\n",
    "start by looking at which of the statements has a higher activation (statements contain location descriptor and pathology already)\n",
    "\n",
    "*Do this without size related questions!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left vs Right: 0.57\n",
      "Mild vs Severe: 0.76\n",
      "Upper vs Lower: 0.72\n",
      "Overall: 0.61\n"
     ]
    }
   ],
   "source": [
    "openi_dataset_path = Path('/vol/biodata/data/chest_xray/OpenI/NLMCXR_png')\n",
    "image_text_reasoning_gt = Path('/vol/biomedic3/bglocker/ugproj2324/nns20/datasets/CheXbench/image_text_reasoning_task')\n",
    "\n",
    "def calculate_mean(similarity_map_top_values):\n",
    "    if len(similarity_map_top_values) == 0:\n",
    "        return 0\n",
    "    return sum([x[2] for x in similarity_map_top_values]) / len(similarity_map_top_values)\n",
    "\n",
    "total_left_vs_right = 0\n",
    "total_mild_vs_severe = 0\n",
    "total_upper_vs_lower = 0\n",
    "\n",
    "correct_left_vs_right = 0\n",
    "correct_mild_vs_severe = 0\n",
    "correct_upper_vs_lower = 0\n",
    "\n",
    "with open(image_text_reasoning_gt) as file:\n",
    "    file.readline() # skip the header\n",
    "    for index, line in enumerate(file.readlines()):\n",
    "        components = line.strip().split(\",\")\n",
    "        image_id = components[2]\n",
    "        question = components[3]\n",
    "        correct_option = components[4]\n",
    "        option_0 = components[5]\n",
    "        option_1 = components[6]\n",
    "\n",
    "        image_path = openi_dataset_path / f\"{image_id}\"\n",
    "\n",
    "        similarity_map_option_0 = image_text_inference.get_similarity_map_from_raw_data(\n",
    "            image_path=image_path,\n",
    "            query_text=option_0,\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "\n",
    "        similarity_map_option_1 = image_text_inference.get_similarity_map_from_raw_data(\n",
    "            image_path=image_path,\n",
    "            query_text=option_1,\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "\n",
    "        top_values_option_0 = calculate_mean(get_top_values(similarity_map_option_0))\n",
    "        top_values_option_1 = calculate_mean(get_top_values(similarity_map_option_1))\n",
    "\n",
    "        predicted = 0 if top_values_option_0 > top_values_option_1 else 1\n",
    "\n",
    "        correct_manual_check = False\n",
    "        if predicted == int(correct_option):\n",
    "            correct_manual_check = True\n",
    "\n",
    "        if (\"left\" in option_0 and \"right\" in option_1) or (\"right\" in option_0 and \"left\" in option_1):\n",
    "            total_left_vs_right += 1\n",
    "            correct_left_vs_right = correct_left_vs_right + 1 if correct_manual_check else correct_left_vs_right\n",
    "\n",
    "        elif (\"mild\" in option_0 and \"severe\" in option_1) or (\"severe\" in option_0 and \"mild\" in option_1):\n",
    "            total_mild_vs_severe += 1\n",
    "            correct_mild_vs_severe = correct_mild_vs_severe + 1 if correct_manual_check else correct_mild_vs_severe\n",
    "        \n",
    "        else:\n",
    "            total_upper_vs_lower += 1\n",
    "            correct_upper_vs_lower = correct_upper_vs_lower + 1 if correct_manual_check else correct_upper_vs_lower\n",
    "        \n",
    "    \n",
    "\n",
    "print(f\"Left vs Right: {correct_left_vs_right/total_left_vs_right:.2f}\")\n",
    "print(f\"Mild vs Severe: {correct_mild_vs_severe/total_mild_vs_severe:.2f}\")\n",
    "print(f\"Upper vs Lower: {correct_upper_vs_lower/total_upper_vs_lower:.2f}\")\n",
    "\n",
    "print(f\"Overall: {(correct_left_vs_right + correct_mild_vs_severe + correct_upper_vs_lower)/(total_left_vs_right + total_mild_vs_severe + total_upper_vs_lower):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assess whether we can localise based only on the pathology description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left vs Right: 0.49\n"
     ]
    }
   ],
   "source": [
    "words_to_remove = {\"left\",\"right\"}\n",
    "\n",
    "correct_manual_check = 0\n",
    "total_left_vs_right = 0\n",
    "\n",
    "with open(image_text_reasoning_gt) as file:\n",
    "    file.readline() # skip the header\n",
    "    for index, line in enumerate(file.readlines()):\n",
    "\n",
    "        components = line.strip().split(\",\")\n",
    "        image_id = components[2]\n",
    "        correct_option = components[4]\n",
    "        option_0 = components[5]\n",
    "        option_1 = components[6]\n",
    "\n",
    "        gt = option_0 if correct_option == 0 else option_1\n",
    "\n",
    "        if \"mild\" in option_0 or \"severe\" in option_0:\n",
    "            continue\n",
    "\n",
    "        phrase = \" \".join([word for word in option_0.split() if word not in words_to_remove])\n",
    "\n",
    "        image_path = openi_dataset_path / f\"{image_id}\"\n",
    "\n",
    "        similarity_map = image_text_inference.get_similarity_map_from_raw_data(\n",
    "            image_path=image_path,\n",
    "            query_text=phrase,\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "\n",
    "        average_x_coordinate = sum([x[0] for x in get_top_values(similarity_map)])/top_n\n",
    "        image = load_image(image_path).convert(\"RGB\")\n",
    "        centre = image.size[0] / 2\n",
    "\n",
    "        prediction = \"right\" if average_x_coordinate < centre else \"left\"\n",
    "\n",
    "        total_left_vs_right += 1\n",
    "        if prediction in gt:\n",
    "            correct_manual_check += 1\n",
    "            \n",
    "print(f\"Left vs Right: {correct_manual_check/total_left_vs_right:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.3 ('multimodal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3ed4ebeb13ecf0e023e91854a12f6980e67b335857d302451652e7fbb2d2298"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
